{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "def processing_text(series_to_process):\n",
    "    new_list = []\n",
    "    tokenizer = RegexpTokenizer(r'(\\w+)')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(series_to_process)):\n",
    "        #TOKENISED ITEM(LONG STRING) IN A LIST\n",
    "        dirty_string = (series_to_process)[i].lower()\n",
    "        words_only = tokenizer.tokenize(dirty_string) #WORDS_ONLY IS A LIST THAT DOESN'T HAVE PUNCTUATION\n",
    "        #LEMMATISE THE ITEMS IN WORDS_ONLY\n",
    "        words_only_lem = [lemmatizer.lemmatize(i) for i in words_only]\n",
    "        #REMOVING STOP WORDS FROM THE LEMMATIZED LIST\n",
    "        words_without_stop = [i for i in words_only_lem if i not in stopwords.words(\"english\")]\n",
    "        #RETURN SEPERATED WORDS INTO LONG STRING\n",
    "        long_string_clean = \" \".join(word for word in words_without_stop)\n",
    "        new_list.append(long_string_clean)\n",
    "    return new_list\n",
    "\n",
    "def processing_label(series_to_process):\n",
    "    new_list = []\n",
    "    for i in range(len(series_to_process)):\n",
    "        if series_to_process[i] in (\"suicide\", \"depression\"):\n",
    "            new_list.append(1)\n",
    "        else:\n",
    "            new_list.append(0)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Depression002.txt\n",
      "Depression003.txt\n",
      "Depression004.txt\n",
      "Depression005.txt\n",
      "Depression006.txt\n",
      "Depression007.txt\n",
      "Depression008.txt\n",
      "Depression009.txt\n",
      "Depression010.txt\n",
      "Depression011.txt\n",
      "Depression012.txt\n",
      "Depression013.txt\n",
      "Depression014.txt\n",
      "Depression015.txt\n",
      "Depression016.txt\n",
      "Depression017.txt\n",
      "Depression018.txt\n",
      "Depression019.txt\n",
      "Depression020.txt\n",
      "Depression021.txt\n",
      "Depression022.txt\n",
      "Depression023.txt\n",
      "Depression024.txt\n",
      "Depression025.txt\n",
      "Depression026.txt\n",
      "Depression027.txt\n",
      "Depression028.txt\n",
      "Depression030.txt\n",
      "Depression031.txt\n",
      "Depression032.txt\n",
      "Depression033.txt\n",
      "Depression034.txt\n",
      "Depression035.txt\n",
      "Depression036.txt\n",
      "Depression037.txt\n",
      "Depression038.txt\n",
      "Depression039.txt\n",
      "Depression040.txt\n",
      "Depression041.txt\n",
      "Depression042.txt\n",
      "Depression043.txt\n",
      "Depression044.txt\n",
      "Depression045.txt\n",
      "Depression046.txt\n",
      "Depression047.txt\n",
      "Depression048.txt\n",
      "Depression049.txt\n",
      "Depression050.txt\n",
      "Depression051.txt\n",
      "Depression052.txt\n",
      "Depression054.txt\n",
      "Depression055.txt\n",
      "Depression057.txt\n",
      "Depression058.txt\n",
      "Depression059.txt\n",
      "Depression061.txt\n",
      "Depression062.txt\n",
      "Depression063.txt\n",
      "Depression066.txt\n",
      "Depression067.txt\n",
      "Depression068.txt\n",
      "Depression069.txt\n",
      "Depression070.txt\n",
      "Depression071.txt\n",
      "Depression072.txt\n",
      "Depression073.txt\n",
      "Depression074.txt\n",
      "Depression075.txt\n",
      "Depression076.txt\n",
      "Depression078.txt\n",
      "Depression079.txt\n",
      "Depression080.txt\n",
      "Depression081.txt\n",
      "Depression082.txt\n",
      "Depression084.txt\n",
      "Depression086.txt\n",
      "Depression087.txt\n",
      "Depression088.txt\n",
      "Depression089.txt\n",
      "Depression090.txt\n",
      "Depression091.txt\n",
      "Depression092.txt\n",
      "Depression093.txt\n",
      "Depression095.txt\n",
      "Depression096.txt\n",
      "Depression097.txt\n",
      "Depression098.txt\n",
      "Depression099.txt\n",
      "Depression100.txt\n",
      "Depression1001.txt\n",
      "Depression1002.txt\n",
      "Depression1003.txt\n",
      "Depression1004.txt\n",
      "Depression1005.txt\n",
      "Depression1006.txt\n",
      "Depression1007.txt\n",
      "Depression1008.txt\n",
      "Depression1009.txt\n",
      "Depression101.txt\n",
      "Depression1010.txt\n",
      "Depression1011.txt\n",
      "Depression1013.txt\n",
      "Depression1015.txt\n",
      "Depression1016.txt\n",
      "Depression1017.txt\n",
      "Depression1018.txt\n",
      "Depression1019.txt\n",
      "Depression102.txt\n",
      "Depression1020.txt\n",
      "Depression1021.txt\n",
      "Depression1022.txt\n",
      "Depression1023.txt\n",
      "Depression1024.txt\n",
      "Depression1025.txt\n",
      "Depression1026.txt\n",
      "Depression1027.txt\n",
      "Depression1028.txt\n",
      "Depression1029.txt\n",
      "Depression103.txt\n",
      "Depression1030.txt\n",
      "Depression1031.txt\n",
      "Depression1032.txt\n",
      "Depression1033.txt\n",
      "Depression1034.txt\n",
      "Depression1035.txt\n",
      "Depression1036.txt\n",
      "Depression1037.txt\n",
      "Depression1038.txt\n",
      "Depression1039.txt\n",
      "Depression104.txt\n",
      "Depression1040.txt\n",
      "Depression1041.txt\n",
      "Depression1042.txt\n",
      "Depression1043.txt\n",
      "Depression1045.txt\n",
      "Depression1046.txt\n",
      "Depression1047.txt\n",
      "Depression1048.txt\n",
      "Depression1049.txt\n",
      "Depression105.txt\n",
      "Depression1050.txt\n",
      "Depression1051.txt\n",
      "Depression1052.txt\n",
      "Depression1053.txt\n",
      "Depression1054.txt\n",
      "Depression1056.txt\n",
      "Depression1057.txt\n",
      "Depression1058.txt\n",
      "Depression1059.txt\n",
      "Depression1060.txt\n",
      "Depression1061.txt\n",
      "Depression1062.txt\n",
      "Depression1063.txt\n",
      "Depression1064.txt\n",
      "Depression1065.txt\n",
      "Depression1066.txt\n",
      "Depression1067.txt\n",
      "Depression1069.txt\n",
      "Depression107.txt\n",
      "Depression1070.txt\n",
      "Depression1071.txt\n",
      "Depression1072.txt\n",
      "Depression1073.txt\n",
      "Depression1074.txt\n",
      "Depression1076.txt\n",
      "Depression1077.txt\n",
      "Depression1078.txt\n",
      "Depression1079.txt\n",
      "Depression108.txt\n",
      "Depression1080.txt\n",
      "Depression1081.txt\n",
      "Depression1082.txt\n",
      "Depression1085.txt\n",
      "Depression1086.txt\n",
      "Depression1087.txt\n",
      "Depression1088.txt\n",
      "Depression1089.txt\n",
      "Depression109.txt\n",
      "Depression1090.txt\n",
      "Depression1091.txt\n",
      "Depression1092.txt\n",
      "Depression1093.txt\n",
      "Depression1094.txt\n",
      "Depression1095.txt\n",
      "Depression1096.txt\n",
      "Depression1097.txt\n",
      "Depression1098.txt\n",
      "Depression1099.txt\n",
      "Depression110.txt\n",
      "Depression1101.txt\n",
      "Depression1102.txt\n",
      "Depression1103.txt\n",
      "Depression1104.txt\n",
      "Depression1105.txt\n",
      "Depression1106.txt\n",
      "Depression1107.txt\n",
      "Depression1108.txt\n",
      "Depression111.txt\n",
      "Depression1110.txt\n",
      "Depression1111.txt\n",
      "Depression1112.txt\n",
      "Depression1113.txt\n",
      "Depression1114.txt\n",
      "Depression1115.txt\n",
      "Depression1116.txt\n",
      "Depression1117.txt\n",
      "Depression1118.txt\n",
      "Depression1119.txt\n",
      "Depression112.txt\n",
      "Depression1120.txt\n",
      "Depression1121.txt\n",
      "Depression1122.txt\n",
      "Depression1123.txt\n",
      "Depression1124.txt\n",
      "Depression1125.txt\n",
      "Depression1126.txt\n",
      "Depression1127.txt\n",
      "Depression1128.txt\n",
      "Depression1129.txt\n",
      "Depression113.txt\n",
      "Depression1130.txt\n",
      "Depression1131.txt\n",
      "Depression1132.txt\n",
      "Depression1133.txt\n",
      "Depression1134.txt\n",
      "Depression1135.txt\n",
      "Depression1136.txt\n",
      "Depression114.txt\n",
      "Depression201.txt\n",
      "Depression202.txt\n",
      "Depression203.txt\n",
      "Depression204.txt\n",
      "Depression205.txt\n",
      "Depression206.txt\n",
      "Depression207.txt\n",
      "Depression208.txt\n",
      "Depression209.txt\n",
      "Depression210.txt\n",
      "Depression211.txt\n",
      "Depression212.txt\n",
      "Depression213.txt\n",
      "Depression214.txt\n",
      "Depression215.txt\n",
      "Depression216.txt\n",
      "Depression217.txt\n",
      "Depression218.txt\n",
      "Depression219.txt\n",
      "Depression222.txt\n",
      "Depression223.txt\n",
      "Depression224.txt\n",
      "Depression225.txt\n",
      "Depression226.txt\n",
      "Depression227.txt\n",
      "Depression229.txt\n",
      "Depression231.txt\n",
      "Depression232.txt\n",
      "Depression233.txt\n",
      "Depression234.txt\n",
      "Depression235.txt\n",
      "Depression236.txt\n",
      "Depression239.txt\n",
      "Depression241.txt\n",
      "Depression242.txt\n",
      "Depression243.txt\n",
      "Depression244.txt\n",
      "Depression246.txt\n",
      "Depression247.txt\n",
      "Depression249.txt\n",
      "Depression250.txt\n",
      "Depression251.txt\n",
      "Depression252.txt\n",
      "Depression255.txt\n",
      "Depression256.txt\n",
      "Depression257.txt\n",
      "Depression258.txt\n",
      "Depression259.txt\n",
      "Depression260.txt\n",
      "Depression261.txt\n",
      "Depression263.txt\n",
      "Depression264.txt\n",
      "Depression265.txt\n",
      "Depression267.txt\n",
      "Depression268.txt\n",
      "Depression269.txt\n",
      "Depression270.txt\n",
      "Depression272.txt\n",
      "Depression274.txt\n",
      "Depression276.txt\n",
      "Depression278.txt\n",
      "Depression279.txt\n",
      "Depression281.txt\n",
      "Depression284.txt\n",
      "Depression285.txt\n",
      "Depression286.txt\n",
      "Depression287.txt\n",
      "Depression288.txt\n",
      "Depression289.txt\n",
      "Depression401.txt\n",
      "Depression402.txt\n",
      "Depression403.txt\n",
      "Depression404.txt\n",
      "Depression405.txt\n",
      "Depression406.txt\n",
      "Depression407.txt\n",
      "Depression408.txt\n",
      "Depression409.txt\n",
      "Depression410.txt\n",
      "Depression411.txt\n",
      "Depression412.txt\n",
      "Depression413.txt\n",
      "Depression414.txt\n",
      "Depression415.txt\n",
      "Depression416.txt\n",
      "Recovery1.txt\n",
      "Recovery10.txt\n",
      "Recovery100.txt\n",
      "Recovery1000.txt\n",
      "Recovery1001.txt\n",
      "Recovery1002.txt\n",
      "Recovery1003.txt\n",
      "Recovery1004.txt\n",
      "Recovery1005.txt\n",
      "Recovery1006.txt\n",
      "Recovery1007.txt\n",
      "Recovery1008.txt\n",
      "Recovery1009.txt\n",
      "Recovery101.txt\n",
      "Recovery1010.txt\n",
      "Recovery1011.txt\n",
      "Recovery1012.txt\n",
      "Recovery1013.txt\n",
      "Recovery1014.txt\n",
      "Recovery1015.txt\n",
      "Recovery1016.txt\n",
      "Recovery1017.txt\n",
      "Recovery1018.txt\n",
      "Recovery1019.txt\n",
      "Recovery102.txt\n",
      "Recovery1021.txt\n",
      "Recovery1022.txt\n",
      "Recovery1023.txt\n",
      "Recovery1024.txt\n",
      "Recovery1025.txt\n",
      "Recovery1026.txt\n",
      "Recovery1027.txt\n",
      "Recovery1028.txt\n",
      "Recovery1029.txt\n",
      "Recovery103.txt\n",
      "Recovery1030.txt\n",
      "Recovery1031.txt\n",
      "Recovery1032.txt\n",
      "Recovery1033.txt\n",
      "Recovery1034.txt\n",
      "Recovery1035.txt\n",
      "Recovery1036.txt\n",
      "Recovery1037.txt\n",
      "Recovery1038.txt\n",
      "Recovery1039.txt\n",
      "Recovery104.txt\n",
      "Recovery1040.txt\n",
      "Recovery105.txt\n",
      "Recovery106.txt\n",
      "Recovery107.txt\n",
      "Recovery108.txt\n",
      "Recovery109.txt\n",
      "Recovery11.txt\n",
      "Recovery110.txt\n",
      "Recovery111.txt\n",
      "Recovery112.txt\n",
      "Recovery113.txt\n",
      "Recovery114.txt\n",
      "Recovery115.txt\n",
      "Recovery116.txt\n",
      "Recovery119.txt\n",
      "Recovery12.txt\n",
      "Recovery120.txt\n",
      "Recovery121.txt\n",
      "Recovery122.txt\n",
      "Recovery123.txt\n",
      "Recovery124.txt\n",
      "Recovery125.txt\n",
      "Recovery126.txt\n",
      "Recovery127.txt\n",
      "Recovery128.txt\n",
      "Recovery129.txt\n",
      "Recovery13.txt\n",
      "Recovery130.txt\n",
      "Recovery131.txt\n",
      "Recovery133.txt\n",
      "Recovery134.txt\n",
      "Recovery135.txt\n",
      "Recovery136.txt\n",
      "Recovery137.txt\n",
      "Recovery138.txt\n",
      "Recovery139.txt\n",
      "Recovery14.txt\n",
      "Recovery140.txt\n",
      "Recovery141.txt\n",
      "Recovery15.txt\n",
      "Recovery17.txt\n",
      "Recovery18.txt\n",
      "Recovery2.txt\n",
      "Recovery201.txt\n",
      "Recovery202.txt\n",
      "Recovery203.txt\n",
      "Recovery204.txt\n",
      "Recovery205.txt\n",
      "Recovery206.txt\n",
      "Recovery207.txt\n",
      "Recovery208.txt\n",
      "Recovery209.txt\n",
      "Recovery210.txt\n",
      "Recovery211.txt\n",
      "Recovery212.txt\n",
      "Recovery213.txt\n",
      "Recovery214.txt\n",
      "Recovery215.txt\n",
      "Recovery216.txt\n",
      "Recovery217.txt\n",
      "Recovery218.txt\n",
      "Recovery219.txt\n",
      "Recovery222.txt\n",
      "Recovery223.txt\n",
      "Recovery224.txt\n",
      "Recovery225.txt\n",
      "Recovery227.txt\n",
      "Recovery228.txt\n",
      "Recovery229.txt\n",
      "Recovery26.txt\n",
      "Recovery27.txt\n",
      "Recovery3.txt\n",
      "Recovery300.txt\n",
      "Recovery301.txt\n",
      "Recovery302.txt\n",
      "Recovery303.txt\n",
      "Recovery304.txt\n",
      "Recovery305.txt\n",
      "Recovery307.txt\n",
      "Recovery308.txt\n",
      "Recovery31.txt\n",
      "Recovery310.txt\n",
      "Recovery311.txt\n",
      "Recovery312.txt\n",
      "Recovery313.txt\n",
      "Recovery314.txt\n",
      "Recovery316.txt\n",
      "Recovery317.txt\n",
      "Recovery318.txt\n",
      "Recovery319.txt\n",
      "Recovery321.txt\n",
      "Recovery323.txt\n",
      "Recovery324.txt\n",
      "Recovery326.txt\n",
      "Recovery327.txt\n",
      "Recovery328.txt\n",
      "Recovery329.txt\n",
      "Recovery33.txt\n",
      "Recovery330.txt\n",
      "Recovery34.txt\n",
      "Recovery36.txt\n",
      "Recovery37.txt\n",
      "Recovery38.txt\n",
      "Recovery39.txt\n",
      "Recovery4.txt\n",
      "Recovery401.txt\n",
      "Recovery402.txt\n",
      "Recovery403.txt\n",
      "Recovery404.txt\n",
      "Recovery405.txt\n",
      "Recovery406.txt\n",
      "Recovery407.txt\n",
      "Recovery408.txt\n",
      "Recovery409.txt\n",
      "Recovery41.txt\n",
      "Recovery410.txt\n",
      "Recovery412.txt\n",
      "Recovery413.txt\n",
      "Recovery414.txt\n",
      "Recovery415.txt\n",
      "Recovery417.txt\n",
      "Recovery418.txt\n",
      "Recovery42.txt\n",
      "Recovery420.txt\n",
      "Recovery421.txt\n",
      "Recovery422.txt\n",
      "Recovery423.txt\n",
      "Recovery424.txt\n",
      "Recovery425.txt\n",
      "Recovery426.txt\n",
      "Recovery427.txt\n",
      "Recovery428.txt\n",
      "Recovery429.txt\n",
      "Recovery43.txt\n",
      "Recovery430.txt\n",
      "Recovery431.txt\n",
      "Recovery432.txt\n",
      "Recovery433.txt\n",
      "Recovery434.txt\n",
      "Recovery435.txt\n",
      "Recovery436.txt\n",
      "Recovery437.txt\n",
      "Recovery438.txt\n",
      "Recovery439.txt\n",
      "Recovery44.txt\n",
      "Recovery440.txt\n",
      "Recovery441.txt\n",
      "Recovery442.txt\n",
      "Recovery443.txt\n",
      "Recovery444.txt\n",
      "Recovery445.txt\n",
      "Recovery446.txt\n",
      "Recovery447.txt\n",
      "Recovery448.txt\n",
      "Recovery449.txt\n",
      "Recovery451.txt\n",
      "Recovery452.txt\n",
      "Recovery453.txt\n",
      "Recovery454.txt\n",
      "Recovery455.txt\n",
      "Recovery456.txt\n",
      "Recovery457.txt\n",
      "Recovery458.txt\n",
      "Recovery459.txt\n",
      "Recovery46.txt\n",
      "Recovery460.txt\n",
      "Recovery461.txt\n",
      "Recovery462.txt\n",
      "Recovery463.txt\n",
      "Recovery464.txt\n",
      "Recovery465.txt\n",
      "Recovery466.txt\n",
      "Recovery467.txt\n",
      "Recovery468.txt\n",
      "Recovery47.txt\n",
      "Recovery474.txt\n",
      "Recovery475.txt\n",
      "Recovery476.txt\n",
      "Recovery477.txt\n",
      "Recovery48.txt\n",
      "Recovery49.txt\n",
      "Recovery5.txt\n",
      "Recovery50.txt\n",
      "Recovery501.txt\n",
      "Recovery502.txt\n",
      "Recovery503.txt\n",
      "Recovery504.txt\n",
      "Recovery506.txt\n",
      "Recovery508.txt\n",
      "Recovery509.txt\n",
      "Recovery51.txt\n",
      "Recovery510.txt\n",
      "Recovery511.txt\n",
      "Recovery512.txt\n",
      "Recovery515.txt\n",
      "Recovery516.txt\n",
      "Recovery517.txt\n",
      "Recovery518.txt\n",
      "Recovery519.txt\n",
      "Recovery52.txt\n",
      "Recovery520.txt\n",
      "Recovery522.txt\n",
      "Recovery523.txt\n",
      "Recovery524.txt\n",
      "Recovery527.txt\n",
      "Recovery528.txt\n",
      "Recovery529.txt\n",
      "Recovery53.txt\n",
      "Recovery530.txt\n",
      "Recovery531.txt\n",
      "Recovery532.txt\n",
      "Recovery534.txt\n",
      "Recovery535.txt\n",
      "Recovery536.txt\n",
      "Recovery538.txt\n",
      "Recovery54.txt\n",
      "Recovery540.txt\n",
      "Recovery541.txt\n",
      "Recovery542.txt\n",
      "Recovery543.txt\n",
      "Recovery544.txt\n",
      "Recovery546.txt\n",
      "Recovery547.txt\n",
      "Recovery548.txt\n",
      "Recovery549.txt\n",
      "Recovery55.txt\n",
      "Recovery550.txt\n",
      "Recovery552.txt\n",
      "Recovery553.txt\n",
      "Recovery555.txt\n",
      "Recovery556.txt\n",
      "Recovery557.txt\n",
      "Recovery559.txt\n",
      "Recovery561.txt\n",
      "Recovery562.txt\n",
      "Recovery563.txt\n",
      "Recovery564.txt\n",
      "Recovery565.txt\n",
      "Recovery566.txt\n",
      "Recovery569.txt\n",
      "Recovery57.txt\n",
      "Recovery571.txt\n",
      "Recovery574.txt\n",
      "Recovery575.txt\n",
      "Recovery576.txt\n",
      "Recovery577.txt\n",
      "Recovery58.txt\n",
      "Recovery580.txt\n",
      "Recovery581.txt\n",
      "Recovery582.txt\n",
      "Recovery583.txt\n",
      "Recovery584.txt\n",
      "Recovery586.txt\n",
      "Recovery588.txt\n",
      "Recovery59.txt\n",
      "Recovery590.txt\n",
      "Recovery591.txt\n",
      "Recovery593.txt\n",
      "Recovery594.txt\n",
      "Recovery595.txt\n",
      "Recovery597.txt\n",
      "Recovery598.txt\n",
      "Recovery599.txt\n",
      "Recovery60.txt\n",
      "Recovery600.txt\n",
      "Recovery601.txt\n",
      "Recovery602.txt\n",
      "        label                                               text\n",
      "0     suicide  Can't stop thinking about dieing\\nI keep think...\n",
      "1     suicide  Please help, happy relationship is broken\\nI a...\n",
      "2     suicide  Stressed beyond my limits\\nA few years back I ...\n",
      "3     suicide  i don't want to go on.\\nI am feeling really lo...\n",
      "4     suicide  I cant take the pain and confusion anymore\\nI ...\n",
      "5     suicide  Hold on please...\\nI have battled Bipolar I fo...\n",
      "6     suicide  don't want to see tomorrow\\nTomorrow is an \"al...\n",
      "7     suicide  It's been years since I thought of this place....\n",
      "8     suicide  Starting to wonder what's the point anymore\\nt...\n",
      "9     suicide  \\nAm I alone?\\nSuicide has been a constant in ...\n",
      "10    suicide  Toxic Relationship\\nHello to all. This is my f...\n",
      "11    suicide  Desperate to Die\\nI am having a particularly r...\n",
      "12    suicide  getting no better...\\nHi everyone...posting ag...\n",
      "13    suicide  Plan to end it all by the end of the summer\\nI...\n",
      "14    suicide  I've lost all hope\\nMy life has always been a ...\n",
      "15    suicide  I need help\\nHello. I'm Sky, and I could reall...\n",
      "16    suicide  Failure.\\nAll my life, I have been treated dif...\n",
      "17    suicide  Why\\nI can't take this anymore. Every time i t...\n",
      "18    suicide  death wish/survivors\\ni am very grateful (to G...\n",
      "19    suicide  dont know what to do\\nSo i'm gay. I'm not real...\n",
      "20    suicide  My Story about me and my BFF\\nHello everyone, ...\n",
      "21    suicide  Loneliness is killer.\\nBefore I get into my pr...\n",
      "22    suicide  From beyond\\nI am in such despair right now......\n",
      "23    suicide  Never gotten this bad before, kind of worried\\...\n",
      "24    suicide  I don't know where to go from here\\nI am 21 ye...\n",
      "25    suicide  I don't know anymore\\nI never knew how bad I w...\n",
      "26    suicide  2am and I can't stop crying\\nIt is 2 am and I ...\n",
      "27    suicide  Should I tell my friends...?\\nHello everyone. ...\n",
      "28    suicide  I am so lost\\nI had a serious suicide attempt ...\n",
      "29    suicide  Hopelessly Depressed\\nIÃ•ve been depressed for ...\n",
      "..        ...                                                ...\n",
      "909  recovery    Thanks\\nI just wanted to say thanks to every...\n",
      "910  recovery    I'm feeling better\\nI spiraled into depressi...\n",
      "911  recovery    Happy Good Day\\n- I woke up early today -- j...\n",
      "912  recovery  Simple But Effective Weight Loss Tips\\nWeight ...\n",
      "913  recovery    Recovering\\nAs some of you know, I have been...\n",
      "914  recovery    This Was a Fun Weekend\\nI went to the Millio...\n",
      "915  recovery    Feeling Beautiful Today\\nI just wanted to po...\n",
      "916  recovery    The sun, and the yard, and the birds\\nAfter ...\n",
      "917  recovery    Drunk but came across a fine idea\\nPurpose i...\n",
      "918  recovery  Realistic Day-To-Day Goals For Someone With De...\n",
      "919  recovery    i want to get to know myself.\\ni want tofeel...\n",
      "920  recovery    I realized what my strength is\\nToday was pr...\n",
      "921  recovery    Breakthrough - Breakdown? - Breakthrough!\\nO...\n",
      "922  recovery    I am so happy!\\nit's been at least a weak wi...\n",
      "923  recovery    Update On My Life...\\nSo here is a lil updat...\n",
      "924  recovery    No booze for 2 weeks = positive frame of min...\n",
      "925  recovery    My Recovery Story\\nWell I was suffering with...\n",
      "926  recovery  It Gets Better\\n\\nI wanted to post this, becau...\n",
      "927  recovery    opted out of fishy game\\nI didn't know where...\n",
      "928  recovery    Getting there, slowly.\\nI'm not fully recove...\n",
      "929  recovery    I think I'm done here. [this time I mean it]...\n",
      "930  recovery    I think I got a job\\nSo I done a trial  shif...\n",
      "931  recovery    knock on wood\\nI've been afraid to admit it,...\n",
      "932  recovery    Click and be amazed..\\nFor anyone of you who...\n",
      "933  recovery    Please know that . . .\\nPlease know that rec...\n",
      "934  recovery    This might be obvious, but it wasn't to me\\n...\n",
      "935  recovery  \\nRelapse: What's Next?!\\nHi Everyone,\\n \\nI h...\n",
      "936  recovery    For once I actually think I did well.\\nTo an...\n",
      "937  recovery    Never thought I would get here ....\\nI wante...\n",
      "938  recovery    I just got my grades back\\nIt's my first yea...\n",
      "\n",
      "[939 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "### Consolidate Depression and Suicide files into one dataframe with 2 columnns (label, text)\n",
    "### Ensured 50/50 breakdown between the 2.\n",
    "### Need a third\n",
    "df = pd.DataFrame(columns=[\"label\", \"text\"])\n",
    "i = 0\n",
    "for file in os.listdir(\"./data/slighly less uncleaned/Suicide\"):\n",
    "    text = open((\"data/slighly less uncleaned/Suicide/\" + file)).read()\n",
    "    df.loc[i] = [\"suicide\"] + [text]\n",
    "    i += 1\n",
    "\n",
    "j = 0\n",
    "while j < i:\n",
    "    file = os.listdir(\"./data/slighly less uncleaned/Depression\")[j]\n",
    "    print(file)\n",
    "    text = open((\"data/slighly less uncleaned/Depression/\" + file)).read()\n",
    "    df.loc[i + j] = [\"depression\"] + [text]\n",
    "    j += 1\n",
    "\n",
    "k = 0\n",
    "while k < j:\n",
    "    file = os.listdir(\"./data/slighly less uncleaned/Recovery\")[k]\n",
    "    print(file)\n",
    "    text = open((\"data/slighly less uncleaned/Recovery/\" + file)).read()\n",
    "    df.loc[i + j + k] = [\"recovery\"] + [text]\n",
    "    k += 1\n",
    "\n",
    "## We know that this dataset is collected from actual surveys of participants.\n",
    "## thus we can safely disregard syntactic or semantic structures of the text.\n",
    "\n",
    "text_values = df[\"text\"]\n",
    "print(df)\n",
    "\n",
    "\n",
    "## we do some simple normalization of the text data.\n",
    "\n",
    "i = 0\n",
    "while i < 3\\\n",
    "        *j:\n",
    "    text = df.loc[i]['text']\n",
    "    text = normalize_document(text)\n",
    "    df.loc[i]['text'] = text\n",
    "    i += 1\n",
    "\n",
    "df[\"text_clean\"] = processing_text(df[\"text\"])\n"
   ]
  },
  {
   "source": [
    "Credits to: https://github.com/hesamuel/goodbye_world/blob/master/code/03_Modelling.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "everyone supported far although wasnt really suicidal didnt care would suddenly died talking saved life putting back track ive depressed really long time roller coaster ev...   \n910  im feeling better spiraled depression recently im beginning see light didnt see psychologist dad put still want see himher though well treat hadnt wouldnt making thread number things help think bi...   \n911  happy good day woke early today oversleeping past two weeks calorie counter account got first time weeks junk food finished yesterday drank remainder juice today hot day degrees first time stayed ...   \n912  simple effective weight loss tips weight loss tip decide much weight need lose people start weight loss journey claiming xyz kilos overweight isnt healthy way project aim weight loss calculate des...   \n913  recovering know depressed suicidal past month combination financial problems social phobia yesterday finally got back money stolen months ago portion alone enough pay bills moms debts repair damag...   \n914  fun weekend went millionaire mind intensive thisweekend second mmi event almost fun say almost acouple ofactivities done last time shocking first exercises seminar focused dealing past events affe...   \n915  feeling beautiful today wanted post cause feel really nice today beautiful mylife got dressed nicely think feel look prettyusually dont think look pretty im getting extra help since last time spok...   \n916  sun yard birds solong darkness see see sun shines see flowers even planted week discovered pair robins made nest yard yesterday saw first ever bluejay wake sometimes ready face day clouds blur vis...   \n917  drunk came across fine idea purpose necessary function society power relies ones perception purpose purpose temporary differentiating spans time situation youfind uncomfortable pensive angry try a...   \n918  realistic daytoday goals someone depression im sure exact right place thread forum im happy moved theres better place last night worked think bothers depression force everything force get force ma...   \n919  want get know want tofeel better goal abuses ive think halted growth able go normal stages emotional development finally good therapist therapist knowledgeable craft trail think get want feeling g...   \n920  realized strength today pretty terrible yesterday slept accident missed morning shift leave class early nauseas nervousness girl sooo sick stomach headches procrastination rage sitting writing rea...   \n921  breakthrough breakdown breakthrough ok ive actually crying letting tears loose past week half havent done couple years really feel like lost focus important life really wanted wanted anything thin...   \n922  happy least weak without one moment want live life fun go fucking live went soundwave music festival seen favourite band entire world seen days later got meet mayday parade boy really changed life...   \n923  update life lil update life since moved parents house since moved november last year things got much betterits like got recharged like battery wierd closer hate hate everything thembut time dont w...   \n924  booze weeks positive frame mind im new ttl wanted share might help someone stuck similar cycle ive battled depression almost life im degree success however recently hospitalised month split boyfri...   \n925  recovery story well suffering depression age growing world sexual mental physical abuse long hard mental health problem deal friends couldnt understand depression family lots problems didnt many p...   \n926  gets better wanted post exactly kind thing needed hear months ago hope helps someone else look months ago like night day last fall given felt completely trapped health issues emotions thought suic...   \n927  opted fishy game didnt know else post think forum appropriate becoming competitive refreshing page waiting fish appear wanted best knew wouldnt possible could try made feel good everytime got fish...   \n928  getting slowly im fully recoverd say im deffinatly feeling better ive learnt deal brain gives hard wont let beat years hiding away seeing life pointless waste precious time life make id made shit ...   \n929  think im done time mean well title says feel longer need account last months really interesting different ways inspiring newer better outlook revolving around music oddly enough remixes getting re...   \n930  think got job done trial shift bar last night hour shift done barwork bar bit class making cocktails hard learn hard remember different techniques one apparently upping cocktail list weeks end shi...   \n931  knock wood ive afraid admit ive feeling bit better recently recovered anything ive kind taking things one step time letting pieces fall may still negativity course even went little rough patch cou...   \n932  click amazed anyone think youre behind ball youre good people feeling typically occurs people somewhat around age feeling able something someone else common occurence quite frankly bums people goo...   \n933  please know please know recovery takes time takes patience perserverance need good communication doctor open therapist make sure doctor therapist communicate well first medications seldom work ope...   \n934  might obvious wasnt supposedly bipolar rapid cycling ive fought shit yrs im odd place right im calm upbeat wo manic feel close think normal people must feel like ever think spent decades simply ex...   \n935  relapse whats next hi everyone havent posted forum many years two months ago suffered relapse major depression decided better try reach currently psychiatrist weaning prozac mg mg try something el...   \n936  actually think well anyone doesnt know today went see someone help mydepression self harm etc paniking week pretty worried going bottle going didnt difficult thing pretty scared woman talking actu...   \n937  never thought would get wanted share story section site months back would read section getting better hope something would grab someone would tell themagic answer magic pill way one wakes one day ...   \n938  got grades back first year university barely got accepted average grade quite bit minimum entry requirement even lot effort summer school didnt manage boost enough minimum however still managed bo...   \n\n                                                                                                                                                                                                  text_clean  \\\n0    cant stop thinking dieing keep thinking suicide dieing even though sometimes dont feel like attempting whatever reason matter happy run mind destiny end life keep thinking suicide driving try end ...   \n1    please help happy relationship broken piece much pain dont want continue anymore loving strong relationship year loving caring man always everything son previous marriage loved much friend always ...   \n2    stressed beyond limit year back lost job due back problem spinal stenosis coupled bulging disk combined fact previous year truck driver back taking beating seated unmoving long period time cause m...   \n3    dont want go feeling really low today dont want go hate everything reason killing dont want kid know stigma parent die suicide reason live though right took mod edit method hoping would give pain ...   \n4    cant take pain confusion anymore dont know could express feeling moment word ill try always suicidal thought mind every single bad event memory would trigger thought mom anchor world helped face e...   \n5    hold please battled bipolar year many hospitalization three suicide attempt two landed icu relatively stable month last week keeping best friend phone reached therapist office expressed suicidal i...   \n6    dont want see tomorrow tomorrow hand meeting division president avoid like plague end long go home bos came cube uncertain term suggested make every effort attend found want trot proof great place...   \n7    year since thought place record longest average post several forum often overlooked maybe absence give meaning thought grudge match tomorrow person unknowingly probably intention saved life introd...   \n8    starting wonder whats point anymore tldr version despite achievement feel like failure miserable life alone dont see point continuing id like start saying currently year old suicidal thought life ...   \n9    alone suicide constant mind year cant say certain triggered depression feel like im finally wit end recently stopped taking zoloft made numb b effexor prozac cymbalta ive taken em make happy go lu...   \n10   toxic relationship hello first time posting site thank letting part last year involved relationship described way toxic started friendship coworkers quickly developed lot biggest problem married i...   \n11   desperate die particularly rough week ive actively suicidal month mean say every moment alive breathing absolute agony wake im mentally disoriented like hangover panic attack anxiety lot depressio...   \n12   getting better hi everyoneposting body dismorphia completely control dont know carry anyone bd site close edge thing stopping lovely partner fab friend cant engage anything mope around day waiting...   \n13   plan end end summer getting affair order mom died brother tired life disappointment cannot stand alone feel like contributed world sad really tired day cannot make till target date obligation keep...   \n14   ive lost hope life always train wreck age root problem started elementary high school constantly bullied brought classmate never friend girl always looked disgust like creepy monster effect selfes...   \n15   need help hello im sky could really use help voice head telling im good enough im really stressed alone right dont know want end know right thing need help want better person dont want leave frien...   \n16   failure life treated differently dont recall im ordinary teenager ambition dream always dream mine join cheerleading could never really try due money difficulty family decided last week since scho...   \n17   cant take anymore every time try get worse living family worse dont care one everything father drug addict even though mom doesnt say disease want pill every second doesnt even take care pill life...   \n18   death wishsurvivors grateful god parent life im bitter anything tend depressed closest feel bored anyway really want kill wouldnt least bc unfinished business importantly bc effect would parent po...   \n19   dont know im gay im really type gay guy know right bat gay could say im straight acting whatever anyways ended coming long time friend mine seemed fine first day later seemed get gay even though t...   \n20   story bff hello everyone today im gonna share story everyone read extremely difficult think im right place well started month ago bff always talking much fun always time needed almost perfect neve...   \n21   loneliness killer get problem want tell know life nowhere near difficult people posting forum knowing others harder doesnt make dealing issue easier though nineteen year old male hope dont sound c...   \n22   beyond despair right tried ring sister didnt answer exam week time tried assignment today didnt even know start stress pressure buried everyone keep telling fine exam want someone say dont went ta...   \n23   never gotten bad kind worried hello everyone first thread mentioned thing life troubling right darkest hour ever ive highly ocd contamination job friend getting parent nerve guilt past wishing cou...   \n24   dont know go year old depression year first cut wrist progressed tried commit suicide time total overdosing third time week ago ended hospital liver failing told going die refused treatment natura...   \n25   dont know anymore never knew bad getting never knew would happen life mess dad hate constantly abusing calling fat stupid waste space good called thing im even person mom always case pressuring no...   \n26   cant stop cry cant stop cry want die dont want hurt friend wonderful friend anyone hope dont know deserve love much much friend care found suicidal friend started secret facebook group use talk im...   \n27   tell friend hello everyone recently decided want end life seeing friend one last time get back college mind made earlier attempt ofmod edit method called change heart mistake left partially usable...   \n28   lost serious suicide attempt several month back several day icu followed facility several family stopped speaking barely saw toddler uninvited holiday checked back facility twice required daily vi...   \n29   hopelessly depressed ive depressed long im sure happy look like even attainable doesnt feel like ever going get better top ive severe anxiety im sure feel anxious time im sleeping much feel mess d...   \n..                                                                                                                                                                                                       ...   \n909  thanks wanted say thanks everyone supported far although wasnt really suicidal didnt care would suddenly died talking saved life putting back track ive depressed really long time roller coaster ev...   \n910  im feeling better spiraled depression recently im beginning see light didnt see psychologist dad put still want see himher though well treat hadnt wouldnt making thread number thing help think big...   \n911  happy good day woke early today oversleeping past two week calorie counter account got first time week junk food finished yesterday drank remainder juice today hot day degree first time stayed sod...   \n912  simple effective weight loss tip weight loss tip decide much weight need lose people start weight loss journey claiming xyz kilo overweight isnt healthy way project aim weight loss calculate desir...   \n913  recovering know depressed suicidal past month combination financial problem social phobia yesterday finally got back money stolen month ago portion alone enough pay bill mom debt repair damage sid...   \n914  fun weekend went millionaire mind intensive thisweekend second mmi event almost fun say almost acouple ofactivities done last time shocking first exercise seminar focused dealing past event affect...   \n915  feeling beautiful today wanted post cause feel really nice today beautiful mylife got dressed nicely think feel look prettyusually dont think look pretty im getting extra help since last time spok...   \n916  sun yard bird solong darkness see see sun shine see flower even planted week discovered pair robin made nest yard yesterday saw first ever bluejay wake sometimes ready face day cloud blur vision n...   \n917  drunk came across fine idea purpose necessary function society power relies one perception purpose purpose temporary differentiating span time situation youfind uncomfortable pensive angry try ass...   \n918  realistic daytoday goal someone depression im sure exact right place thread forum im happy moved better place last night worked think bother depression force everything force get force make breakf...   \n919  want get know want tofeel better goal abuse ive think halted growth able go normal stage emotional development finally good therapist therapist knowledgeable craft trail think get want feeling goo...   \n920  realized strength today pretty terrible yesterday slept accident missed morning shift leave class early nausea nervousness girl sooo sick stomach headches procrastination rage sitting writing real...   \n921  breakthrough breakdown breakthrough ok ive actually cry letting tear loose past week half havent done couple year really feel like lost focus important life really wanted wanted anything think pro...   \n922  happy least weak without one moment want live life fun go fucking live went soundwave music festival seen favourite band entire world seen day later got meet mayday parade boy really changed life ...   \n923  update life lil update life since moved parent house since moved november last year thing got much betterits like got recharged like battery wierd closer hate hate everything thembut time dont wan...   \n924  booze week positive frame mind im new ttl wanted share might help someone stuck similar cycle ive battled depression almost life im degree success however recently hospitalised month split boyfrie...   \n925  recovery story well suffering depression age growing world sexual mental physical abuse long hard mental health problem deal friend couldnt understand depression family lot problem didnt many peop...   \n926  get better wanted post exactly kind thing needed hear month ago hope help someone else look month ago like night day last fall given felt completely trapped health issue emotion thought suicide co...   \n927  opted fishy game didnt know else post think forum appropriate becoming competitive refreshing page waiting fish appear wanted best knew wouldnt possible could try made feel good everytime got fish...   \n928  getting slowly im fully recoverd say im deffinatly feeling better ive learnt deal brain give hard wont let beat year hiding away seeing life pointless waste precious time life make id made shit ob...   \n929  think im done time mean well title say feel longer need account last month really interesting different way inspiring newer better outlook revolving around music oddly enough remixes getting remix...   \n930  think got job done trial shift bar last night hour shift done barwork bar bit class making cocktail hard learn hard remember different technique one apparently upping cocktail list week end shift ...   \n931  knock wood ive afraid admit ive feeling bit better recently recovered anything ive kind taking thing one step time letting piece fall may still negativity course even went little rough patch coupl...   \n932  click amazed anyone think youre behind ball youre good people feeling typically occurs people somewhat around age feeling able something someone else common occurence quite frankly bum people good...   \n933  please know please know recovery take time take patience perserverance need good communication doctor open therapist make sure doctor therapist communicate well first medication seldom work open c...   \n934  might obvious wasnt supposedly bipolar rapid cycling ive fought shit yr im odd place right im calm upbeat wo manic feel close think normal people must feel like ever think spent decade simply exha...   \n935  relapse whats next hi everyone havent posted forum many year two month ago suffered relapse major depression decided better try reach currently psychiatrist weaning prozac mg mg try something else...   \n936  actually think well anyone doesnt know today went see someone help mydepression self harm etc paniking week pretty worried going bottle going didnt difficult thing pretty scared woman talking actu...   \n937  never thought would get wanted share story section site month back would read section getting better hope something would grab someone would tell themagic answer magic pill way one wake one day re...   \n938  got grade back first year university barely got accepted average grade quite bit minimum entry requirement even lot effort summer school didnt manage boost enough minimum however still managed boo...   \n\n     is_bad  \n0         1  \n1         1  \n2         1  \n3         1  \n4         1  \n5         1  \n6         1  \n7         1  \n8         1  \n9         1  \n10        1  \n11        1  \n12        1  \n13        1  \n14        1  \n15        1  \n16        1  \n17        1  \n18        1  \n19        1  \n20        1  \n21        1  \n22        1  \n23        1  \n24        1  \n25        1  \n26        1  \n27        1  \n28        1  \n29        1  \n..      ...  \n909       0  \n910       0  \n911       0  \n912       0  \n913       0  \n914       0  \n915       0  \n916       0  \n917       0  \n918       0  \n919       0  \n920       0  \n921       0  \n922       0  \n923       0  \n924       0  \n925       0  \n926       0  \n927       0  \n928       0  \n929       0  \n930       0  \n931       0  \n932       0  \n933       0  \n934       0  \n935       0  \n936       0  \n937       0  \n938       0  \n\n[939 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"is_bad\"] = processing_label(df[\"label\"])\n",
    "print(df)"
   ]
  },
  {
   "source": [
    "## Baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "df[\"is_bad\"].mean()"
   ]
  },
  {
   "source": [
    "## Finding a Production Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING A FUNCTION THAT WILL RUN MULTIPLE MODELS AND GRIDSEARCH FOR BEST PARAMETERS\n",
    "\n",
    "def gridsearch_multi(steps_titles, steps_list, pipe_params):\n",
    "    \n",
    "    #DEFINING X and y\n",
    "    X = df[\"text_clean\"]\n",
    "    y = df['is_bad']\n",
    "    #TRAIN-TEST SPLIT\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    # DATAFRAME TO DISPLAY RESULTS\n",
    "    gs_results = pd.DataFrame(columns=['model','AUC Score', 'precision', 'recall (sensitivity)', \n",
    "                                       'best_params', 'best score', 'confusion matrix', \n",
    "                                       'train_accuracy','test_accuracy','baseline_accuracy',\n",
    "                                       'specificity', 'f1-score'])\n",
    "\n",
    "    # FOR LOOP THROUGH STEPS LIST\n",
    "    for i in range(len(steps_list)):\n",
    "        # INSTATIATE PIPELINE\n",
    "        pipe = Pipeline(steps=steps_list[i])\n",
    "        # INSTANTIATE GRIDSEARCHCV WITH PARAMETER ARGUMENT\n",
    "        gs = GridSearchCV(pipe, pipe_params[i], cv=3) \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        #GETTING PREDICTIONS FROM MODEL\n",
    "        pred = gs.predict(X_test)\n",
    "        \n",
    "        # DEFINE CONFUSION MATRIX ELEMENTS\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "        \n",
    "        #CREATING A DICTIONARY FROM THE CLASSIFICATION REPORT(WE'LL DRAW SOME METRICS FROM HERE)\n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "        \n",
    "        #CALCULATING AREA UNDER THE CURVE\n",
    "        gs.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in gs.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        #DEFINE DATAFRAME COLUMNS\n",
    "        model_results = {}\n",
    "        model_results['model'] = steps_titles[i]\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['best params'] = gs.best_params_\n",
    "        model_results['best score'] = gs.best_score_\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = gs.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = gs.score(X_test, y_test)\n",
    "        model_results['baseline accuracy'] = 2/3\n",
    "        \n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "        #APPEND RESULTS TO A NICE DATAFRAME\n",
    "        df_list.append(model_results) \n",
    "        pd.set_option(\"display.max_colwidth\", 200)\n",
    "    return (pd.DataFrame(df_list)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   AUC Score  baseline accuracy  \\\n",
       "0       0.80               0.67   \n",
       "1       0.58               0.67   \n",
       "2       0.78               0.67   \n",
       "\n",
       "                                                                                                             best params  \\\n",
       "0  {'cv__max_df': 0.25, 'cv__max_features': 50, 'cv__min_df': 3, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "1   {'cv__max_df': 0.3, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "\n",
       "   best score                           confusion matrix  f1-score  \\\n",
       "0        0.71  {'TP': 131, 'FP': 33, 'TN': 45, 'FN': 26}      0.75   \n",
       "1        0.68  {'TP': 120, 'FP': 52, 'TN': 26, 'FN': 37}      0.61   \n",
       "2        0.71  {'TP': 143, 'FP': 41, 'TN': 37, 'FN': 14}      0.75   \n",
       "\n",
       "                model  precision  recall (sensitivity)  specificity  \\\n",
       "0      cvec+ multi_nb       0.74                  0.75         0.58   \n",
       "1     cvec + ss + knn       0.60                  0.62         0.33   \n",
       "2  cvec + ss + logreg       0.76                  0.77         0.47   \n",
       "\n",
       "   test accuracy  train accuracy  \n",
       "0           0.75            0.74  \n",
       "1           0.62            0.78  \n",
       "2           0.77            0.77  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AUC Score</th>\n      <th>baseline accuracy</th>\n      <th>best params</th>\n      <th>best score</th>\n      <th>confusion matrix</th>\n      <th>f1-score</th>\n      <th>model</th>\n      <th>precision</th>\n      <th>recall (sensitivity)</th>\n      <th>specificity</th>\n      <th>test accuracy</th>\n      <th>train accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.80</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.25, 'cv__max_features': 50, 'cv__min_df': 3, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 131, 'FP': 33, 'TN': 45, 'FN': 26}</td>\n      <td>0.75</td>\n      <td>cvec+ multi_nb</td>\n      <td>0.74</td>\n      <td>0.75</td>\n      <td>0.58</td>\n      <td>0.75</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.58</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.3, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.68</td>\n      <td>{'TP': 120, 'FP': 52, 'TN': 26, 'FN': 37}</td>\n      <td>0.61</td>\n      <td>cvec + ss + knn</td>\n      <td>0.60</td>\n      <td>0.62</td>\n      <td>0.33</td>\n      <td>0.62</td>\n      <td>0.78</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.78</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 143, 'FP': 41, 'TN': 37, 'FN': 14}</td>\n      <td>0.75</td>\n      <td>cvec + ss + logreg</td>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>0.47</td>\n      <td>0.77</td>\n      <td>0.77</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "#USING THE FUNCTION WITH COUNT VECTORIZOR\n",
    "\n",
    "# EMPTY LIST THAT WILL HOLD RESULTS\n",
    "df_list=[]\n",
    "\n",
    "# LIST OF MODELS\n",
    "steps_titles = ['cvec+ multi_nb','cvec + ss + knn','cvec + ss + logreg']\n",
    "\n",
    "# CODE FOR PIPELINE TO INSTATIATE MODELS\n",
    "steps_list = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# LIST OF PARAMETER DICTIONARIES\n",
    "pipe_params = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]}\n",
    "]\n",
    "\n",
    "#RUNNING THE FUNCTION\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   AUC Score  baseline accuracy  \\\n",
       "0       0.80               0.67   \n",
       "1       0.58               0.67   \n",
       "2       0.78               0.67   \n",
       "3       0.82               0.67   \n",
       "4       0.63               0.67   \n",
       "5       0.79               0.67   \n",
       "\n",
       "                                                                                                             best params  \\\n",
       "0  {'cv__max_df': 0.25, 'cv__max_features': 50, 'cv__min_df': 3, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "1   {'cv__max_df': 0.3, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "3   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 3, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4  {'tv__max_df': 0.25, 'tv__max_features': 30, 'tv__min_df': 3, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5  {'tv__max_df': 0.25, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "\n",
       "   best score                           confusion matrix  f1-score  \\\n",
       "0        0.71  {'TP': 131, 'FP': 33, 'TN': 45, 'FN': 26}      0.75   \n",
       "1        0.68  {'TP': 120, 'FP': 52, 'TN': 26, 'FN': 37}      0.61   \n",
       "2        0.71  {'TP': 143, 'FP': 41, 'TN': 37, 'FN': 14}      0.75   \n",
       "3        0.70   {'TP': 154, 'FP': 64, 'TN': 14, 'FN': 3}      0.65   \n",
       "4        0.68  {'TP': 120, 'FP': 44, 'TN': 34, 'FN': 37}      0.65   \n",
       "5        0.71  {'TP': 140, 'FP': 42, 'TN': 36, 'FN': 17}      0.73   \n",
       "\n",
       "                model  precision  recall (sensitivity)  specificity  \\\n",
       "0      cvec+ multi_nb       0.74                  0.75         0.58   \n",
       "1     cvec + ss + knn       0.60                  0.62         0.33   \n",
       "2  cvec + ss + logreg       0.76                  0.77         0.47   \n",
       "3     tvec + multi_nb       0.75                  0.71         0.18   \n",
       "4     tvec + ss + knn       0.65                  0.66         0.44   \n",
       "5  tvec + ss + logreg       0.74                  0.75         0.46   \n",
       "\n",
       "   test accuracy  train accuracy  \n",
       "0           0.75            0.74  \n",
       "1           0.62            0.78  \n",
       "2           0.77            0.77  \n",
       "3           0.71            0.72  \n",
       "4           0.66            0.77  \n",
       "5           0.75            0.74  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AUC Score</th>\n      <th>baseline accuracy</th>\n      <th>best params</th>\n      <th>best score</th>\n      <th>confusion matrix</th>\n      <th>f1-score</th>\n      <th>model</th>\n      <th>precision</th>\n      <th>recall (sensitivity)</th>\n      <th>specificity</th>\n      <th>test accuracy</th>\n      <th>train accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.80</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.25, 'cv__max_features': 50, 'cv__min_df': 3, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 131, 'FP': 33, 'TN': 45, 'FN': 26}</td>\n      <td>0.75</td>\n      <td>cvec+ multi_nb</td>\n      <td>0.74</td>\n      <td>0.75</td>\n      <td>0.58</td>\n      <td>0.75</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.58</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.3, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.68</td>\n      <td>{'TP': 120, 'FP': 52, 'TN': 26, 'FN': 37}</td>\n      <td>0.61</td>\n      <td>cvec + ss + knn</td>\n      <td>0.60</td>\n      <td>0.62</td>\n      <td>0.33</td>\n      <td>0.62</td>\n      <td>0.78</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.78</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 143, 'FP': 41, 'TN': 37, 'FN': 14}</td>\n      <td>0.75</td>\n      <td>cvec + ss + logreg</td>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>0.47</td>\n      <td>0.77</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.82</td>\n      <td>0.67</td>\n      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 3, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n      <td>0.70</td>\n      <td>{'TP': 154, 'FP': 64, 'TN': 14, 'FN': 3}</td>\n      <td>0.65</td>\n      <td>tvec + multi_nb</td>\n      <td>0.75</td>\n      <td>0.71</td>\n      <td>0.18</td>\n      <td>0.71</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.63</td>\n      <td>0.67</td>\n      <td>{'tv__max_df': 0.25, 'tv__max_features': 30, 'tv__min_df': 3, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n      <td>0.68</td>\n      <td>{'TP': 120, 'FP': 44, 'TN': 34, 'FN': 37}</td>\n      <td>0.65</td>\n      <td>tvec + ss + knn</td>\n      <td>0.65</td>\n      <td>0.66</td>\n      <td>0.44</td>\n      <td>0.66</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.79</td>\n      <td>0.67</td>\n      <td>{'tv__max_df': 0.25, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 140, 'FP': 42, 'TN': 36, 'FN': 17}</td>\n      <td>0.73</td>\n      <td>tvec + ss + logreg</td>\n      <td>0.74</td>\n      <td>0.75</td>\n      <td>0.46</td>\n      <td>0.75</td>\n      <td>0.74</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "#USING THE FUNCTION WITH TFID VECTORIZOR\n",
    "\n",
    "# LIST OF MODELS\n",
    "steps_titles = ['tvec + multi_nb','tvec + ss + knn','tvec + ss + logreg']\n",
    "\n",
    "# CODE FOR PIPELINE TO INSTATIATE MODELS\n",
    "steps_list = [ \n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# LIST OF PARAMETER DICTIONARIES\n",
    "pipe_params = [\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "#RUNNING THE FUNCTION\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   AUC Score  baseline accuracy  \\\n",
       "0       0.80               0.67   \n",
       "1       0.58               0.67   \n",
       "2       0.78               0.67   \n",
       "3       0.82               0.67   \n",
       "4       0.63               0.67   \n",
       "5       0.79               0.67   \n",
       "6       0.70               0.67   \n",
       "7       0.51               0.67   \n",
       "8       0.88               0.67   \n",
       "\n",
       "                                                                                                             best params  \\\n",
       "0  {'cv__max_df': 0.25, 'cv__max_features': 50, 'cv__min_df': 3, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "1   {'cv__max_df': 0.3, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}   \n",
       "3   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 3, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4  {'tv__max_df': 0.25, 'tv__max_features': 30, 'tv__min_df': 3, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5  {'tv__max_df': 0.25, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "6                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "7                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "8                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "\n",
       "   best score                           confusion matrix  f1-score  \\\n",
       "0        0.71  {'TP': 131, 'FP': 33, 'TN': 45, 'FN': 26}      0.75   \n",
       "1        0.68  {'TP': 120, 'FP': 52, 'TN': 26, 'FN': 37}      0.61   \n",
       "2        0.71  {'TP': 143, 'FP': 41, 'TN': 37, 'FN': 14}      0.75   \n",
       "3        0.70   {'TP': 154, 'FP': 64, 'TN': 14, 'FN': 3}      0.65   \n",
       "4        0.68  {'TP': 120, 'FP': 44, 'TN': 34, 'FN': 37}      0.65   \n",
       "5        0.71  {'TP': 140, 'FP': 42, 'TN': 36, 'FN': 17}      0.73   \n",
       "6        0.67    {'TP': 157, 'FP': 78, 'TN': 0, 'FN': 0}      0.54   \n",
       "7        0.67    {'TP': 157, 'FP': 78, 'TN': 0, 'FN': 0}      0.54   \n",
       "8        0.77   {'TP': 151, 'FP': 40, 'TN': 38, 'FN': 6}      0.79   \n",
       "\n",
       "                model  precision  recall (sensitivity)  specificity  \\\n",
       "0      cvec+ multi_nb       0.74                  0.75         0.58   \n",
       "1     cvec + ss + knn       0.60                  0.62         0.33   \n",
       "2  cvec + ss + logreg       0.76                  0.77         0.47   \n",
       "3     tvec + multi_nb       0.75                  0.71         0.18   \n",
       "4     tvec + ss + knn       0.65                  0.66         0.44   \n",
       "5  tvec + ss + logreg       0.74                  0.75         0.46   \n",
       "6     hvec + multi_nb       0.45                  0.67         0.00   \n",
       "7     hvec + ss + knn       0.45                  0.67         0.00   \n",
       "8  hvec + ss + logreg       0.81                  0.80         0.49   \n",
       "\n",
       "   test accuracy  train accuracy  \n",
       "0           0.75            0.74  \n",
       "1           0.62            0.78  \n",
       "2           0.77            0.77  \n",
       "3           0.71            0.72  \n",
       "4           0.66            0.77  \n",
       "5           0.75            0.74  \n",
       "6           0.67            0.67  \n",
       "7           0.67            0.68  \n",
       "8           0.80            1.00  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AUC Score</th>\n      <th>baseline accuracy</th>\n      <th>best params</th>\n      <th>best score</th>\n      <th>confusion matrix</th>\n      <th>f1-score</th>\n      <th>model</th>\n      <th>precision</th>\n      <th>recall (sensitivity)</th>\n      <th>specificity</th>\n      <th>test accuracy</th>\n      <th>train accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.80</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.25, 'cv__max_features': 50, 'cv__min_df': 3, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 131, 'FP': 33, 'TN': 45, 'FN': 26}</td>\n      <td>0.75</td>\n      <td>cvec+ multi_nb</td>\n      <td>0.74</td>\n      <td>0.75</td>\n      <td>0.58</td>\n      <td>0.75</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.58</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.3, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.68</td>\n      <td>{'TP': 120, 'FP': 52, 'TN': 26, 'FN': 37}</td>\n      <td>0.61</td>\n      <td>cvec + ss + knn</td>\n      <td>0.60</td>\n      <td>0.62</td>\n      <td>0.33</td>\n      <td>0.62</td>\n      <td>0.78</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.78</td>\n      <td>0.67</td>\n      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 143, 'FP': 41, 'TN': 37, 'FN': 14}</td>\n      <td>0.75</td>\n      <td>cvec + ss + logreg</td>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>0.47</td>\n      <td>0.77</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.82</td>\n      <td>0.67</td>\n      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 3, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n      <td>0.70</td>\n      <td>{'TP': 154, 'FP': 64, 'TN': 14, 'FN': 3}</td>\n      <td>0.65</td>\n      <td>tvec + multi_nb</td>\n      <td>0.75</td>\n      <td>0.71</td>\n      <td>0.18</td>\n      <td>0.71</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.63</td>\n      <td>0.67</td>\n      <td>{'tv__max_df': 0.25, 'tv__max_features': 30, 'tv__min_df': 3, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n      <td>0.68</td>\n      <td>{'TP': 120, 'FP': 44, 'TN': 34, 'FN': 37}</td>\n      <td>0.65</td>\n      <td>tvec + ss + knn</td>\n      <td>0.65</td>\n      <td>0.66</td>\n      <td>0.44</td>\n      <td>0.66</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.79</td>\n      <td>0.67</td>\n      <td>{'tv__max_df': 0.25, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n      <td>0.71</td>\n      <td>{'TP': 140, 'FP': 42, 'TN': 36, 'FN': 17}</td>\n      <td>0.73</td>\n      <td>tvec + ss + logreg</td>\n      <td>0.74</td>\n      <td>0.75</td>\n      <td>0.46</td>\n      <td>0.75</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.70</td>\n      <td>0.67</td>\n      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n      <td>0.67</td>\n      <td>{'TP': 157, 'FP': 78, 'TN': 0, 'FN': 0}</td>\n      <td>0.54</td>\n      <td>hvec + multi_nb</td>\n      <td>0.45</td>\n      <td>0.67</td>\n      <td>0.00</td>\n      <td>0.67</td>\n      <td>0.67</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.51</td>\n      <td>0.67</td>\n      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n      <td>0.67</td>\n      <td>{'TP': 157, 'FP': 78, 'TN': 0, 'FN': 0}</td>\n      <td>0.54</td>\n      <td>hvec + ss + knn</td>\n      <td>0.45</td>\n      <td>0.67</td>\n      <td>0.00</td>\n      <td>0.67</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.88</td>\n      <td>0.67</td>\n      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n      <td>0.77</td>\n      <td>{'TP': 151, 'FP': 40, 'TN': 38, 'FN': 6}</td>\n      <td>0.79</td>\n      <td>hvec + ss + logreg</td>\n      <td>0.81</td>\n      <td>0.80</td>\n      <td>0.49</td>\n      <td>0.80</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "#USING THE FUNCTION WITH HASHING VECTORIZOR\n",
    "\n",
    "# LIST OF MODELS\n",
    "steps_titles = ['hvec + multi_nb','hvec + ss + knn','hvec + ss + logreg']\n",
    "\n",
    "# CODE FOR PIPELINE TO INSTATIATE MODELS\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# LIST OF PARAMETER DICTIONARIES\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]}\n",
    "]   \n",
    "\n",
    "#RUNNING THE FUNCTION\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "source": [
    "## Production Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACCURACY: 0.8\nAUC SCORE: 0.8534215253960478\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CHECKING SCORES OF THE OPTIMISED MODEL USING TEST DATA\n",
    "#DEFINING X and y\n",
    "X = df[\"text_clean\"]\n",
    "y = df['is_bad']\n",
    "#TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n",
    "X_train_tvec = tvec_optimised.fit_transform(X_train).todense()\n",
    "X_test_tvec = tvec_optimised.transform(X_test).todense()\n",
    "\n",
    "#SAVE TO PICKLE\n",
    "import pickle\n",
    "pickle.dump(tvec_optimised, open(\"vectorizer.pickle\", \"wb\"))\n",
    "\n",
    "#FINDING THE ACCURACY SCORE ON THE TEST DATA\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_tvec, y_train)\n",
    "accuracy = log_reg.score(X_test_tvec, y_test)\n",
    "\n",
    "#CALCULATING AREA UNDER THE CURVE\n",
    "\n",
    "pred_proba = [i[1] for i in log_reg.predict_proba(X_test_tvec)] \n",
    "auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "print(\"ACCURACY: {}\\nAUC SCORE: {}\".format(accuracy, auc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "test = [\"I think it's overfitted\"]\n",
    "vectorizer = pickle.load(open(\"vectorizer.pickle\", \"rb\"))\n",
    "new_X = vectorizer.transform(test).todense()\n",
    "result = log_reg.predict(new_X)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}